<section id="experience" class="home-section wg-pages">
    <div class="container">
        <div class="row">
            <div class="col-12 col-lg-4 section-heading">
                <h1>Research Experience</h1>
            </div>
            <div class="col-12 col-lg-8">
                <div class="card-simple">
                    <h3 class="article-title mb-1 mt-3">
                        LogShield: A Transformer-based APT Detection System Leveraging Self-Attention
                    </h3>
                    
                    <div class="article-style">
                        <p> 
                            <strong>Abstract—</strong>A framework designed to detect APT attack patterns leveraging the power of self-attention in
                            transformers. We incorporate customized embedding layers to effectively capture the context of event sequences
                            derived from provenance graphs. While acknowledging the computational overhead associated with training
                            transformer networks, our framework surpasses existing LSTM and Language models regarding APT detection
                            performance. We integrated the model parameters and training procedure from the RoBERTa model and conducted
                            extensive experiments on well-known APT datasets (DARPA OpTC and DARPA TC E3). Our framework achieved
                            superior F1 scores of 98\% and 95\% on the two datasets respectively, surpassing the F1 scores of 96\% and 94\%
                            obtained by LSTM models. Our findings suggest that LogShield's performance benefits from larger datasets and
                            demonstrates its potential for generalization across diverse domains.
                        </p>
                        <p>
                            <strong>Status</strong> : Under review, <a href="https://arxiv.org/pdf/2311.05733.pdf">ArXiv Version</a>
                        </p>
                    </div>
                  
                </div>
                <div class="card-simple">
                    <h3 class="article-title mb-1 mt-3">
                        Protocol State Fuzzing of 4G LTE Devices
                    </h3>
                    
                    <div class="article-style">
                        <p>
                            We will be analyzing several 4G LTE COTs devices to detect any noncompliant behavior that deviates from the
                            protocol specification. Active Automata Learning is being used to infer finite state machine from 4G LTE protocol
                            implementation. The deviation will be detected from the learned FSMs and the protocol specifications. This is a collaboration 
                            project under the supervision of <a href="https://mshohrabhossain.buet.ac.bd/">Professor Md. Shohrab Hossain</a>. 
                        </p>
                        <p>
                            <p>
                                <strong>Status</strong> : Ongoing
                            </p>
                        </p>
                    </div>

                </div>

                <div class="card-simple">
                    <h3 class="article-title mb-1 mt-3">
                        Bangla Grammatical Error Detection Leveraging Transformer-based Token Classification
                    </h3>
                    
                    <div class="article-style">
                        <p>
                            <strong>Abstract—</strong> Bangla is the seventh most spoken language by a total number of speakers in the world, and yet the
                            development of an automated grammar checker in this language is an understudied problem. Bangla grammatical
                            error detection is a task of detecting sub-strings of a Bangla text that contain grammatical, punctuation, or spelling
                            errors, which is crucial for developing an automated Bangla typing assistant. Our approach involves breaking down
                            the task as a token classification problem and utilizing state-of-the-art transformer-based models. Finally, we combine
                            the output of these models and apply rule-based post-processing to generate a more reliable and comprehensive
                            result. Our system is evaluated on a dataset consisting of over 25,000 texts from various sources. Our best model
                            achieves a Levenshtein distance score of 1.04. Finally, we provide a detailed analysis of different components of our
                            system.
                        </p>
                        <p>
                            <strong>Status</strong> : In Preparation for ACL Workshop, <a href="https://drive.google.com/file/d/1LvHrXdx8e5mtFrj-m9CSLLZvCsHb4g1I/view?usp=sharing">[pdf]</a>    
                        </p>
                    </div>
                </div>
                
            </div>
        </div>
        </br>
        </br>
        </br>
    </div>
</section>